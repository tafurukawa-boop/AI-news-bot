# -*- coding: utf-8 -*-
"""AIãƒ‹ãƒ¥ãƒ¼ã‚¹ç”¨

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B8Yqpyos50nhN8YxKg2pmQ7A-BBepk7V
"""

import feedparser
import openai
from slack_sdk.webhook import WebhookClient
import os

# --- APIã‚­ãƒ¼ã¯GitHub Secretsã‹ã‚‰å–å¾— ---
openai.api_key = os.getenv("OPENAI_API_KEY")
slack_webhook_url = os.getenv("SLACK_WEBHOOK_URL")

# --- ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ï¼ˆRSSã‹ã‚‰å–å¾—ï¼‰ ---
def fetch_rss(feed_url, max_items=2):
    feed = feedparser.parse(feed_url)
    articles = []
    for entry in feed.entries[:max_items]:
        articles.append({
            "title": entry.title,
            "link": entry.link,
            "summary": entry.get("summary", "") or entry.get("description", "")
        })
    return articles

# --- GPTã«ã‚ˆã‚‹ä»•åˆ†ã‘ï¼†è¦ç´„ ---
def classify_and_summarize(articles):
    articles_text = "\n".join([f"- {a['title']} ({a['link']}) {a['summary']}" for a in articles])
    prompt = f"""
ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’ã€Œæ³¨ç›®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€ã¨ã€Œãã®ä»–ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€ã«åˆ†é¡ã—ã€æŒ‡å®šãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚

### æ³¨ç›®ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆæ—¥æœ¬ãƒ»Indeedãƒ»äººææ¥­ç•Œã«é–¢ä¿‚ã™ã‚‹ã‚‚ã®ï¼‰
âš¡ åˆºæ¿€ãƒ•ãƒ¬ãƒ¼ã‚º
ğŸ“° ã‚¿ã‚¤ãƒˆãƒ«
âœ… ä¸–ã®ä¸­ã¯ã©ã†å¤‰ã‚ã‚‹ã‹ï¼Ÿ
ğŸ‘‰ æ—¥æœ¬å›½å†…ã§ã®æ¡ç”¨ãƒ»äººææ¥­ç•Œã¸ã®å½±éŸ¿
ğŸŸ¢ CSè¦–ç‚¹ã§ã®ç¤ºå”†ï¼ˆAIã§å¯èƒ½ã«ãªã£ãŸã“ã¨ãŒæ—¢å­˜ãƒ•ãƒ­ãƒ¼ã‚’ã©ã†å¤‰ãˆã‚‹ã‹ã€å…ˆèª­ã¿ã—ã¦æº–å‚™ã™ã¹ãã“ã¨ï¼‰
ğŸ’¡ ä»–ä»£ç†åº—ãƒ»ä»–éƒ¨ç½²ã¨ã®å·®åˆ¥åŒ–ãƒã‚¤ãƒ³ãƒˆ
ğŸ”® è¿‘æœªæ¥äºˆæ¸¬

### ãã®ä»–ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆãã‚Œä»¥å¤–ã®æµ·å¤–ã‚„å‘¨è¾ºæƒ…å ±ï¼‰
ğŸ“Œ ã‚¿ã‚¤ãƒˆãƒ«
â†’ ä¸€è¨€ã‚³ãƒ¡ãƒ³ãƒˆ
ğŸ”— URL

è¨˜äº‹ä¸€è¦§ï¼š
{articles_text}
"""
    res = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
    )
    return res.choices[0].message.content

# --- SlackæŠ•ç¨¿ ---
def post_to_slack(message):
    webhook = WebhookClient(slack_webhook_url)
    webhook.send(text=message)

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
if __name__ == "__main__":
    rss_urls = [
        # ğŸŒ æµ·å¤–
        "https://techcrunch.com/tag/artificial-intelligence/feed/",
        "https://venturebeat.com/category/ai/feed/",
        "https://www.technologyreview.com/feed/",
        "https://www.theverge.com/artificial-intelligence/rss/index.xml",
        "http://feeds.bbci.co.uk/news/technology/rss.xml",

        # ğŸ‡¯ğŸ‡µ å›½å†…
        "https://www.nikkei.com/rss/technology.rdf",
        "https://rss.itmedia.co.jp/rss/2.0/news_bursts.xml",
        "https://www.hrpro.co.jp/rss/",
        "https://jinjibu.jp/rss/news.xml",
        "https://ai-scholar.tech/feed"
    ]

    all_articles = []
    for url in rss_urls:
        all_articles.extend(fetch_rss(url, max_items=1))  # 1è¨˜äº‹ã ã‘å–ã‚‹ã‚ˆã†èª¿æ•´

    print(f"åé›†è¨˜äº‹æ•°: {len(all_articles)}")

    summary = classify_and_summarize(all_articles)
    post_to_slack(summary)
    print("Slackã«é€ä¿¡ã—ã¾ã—ãŸ âœ…")
